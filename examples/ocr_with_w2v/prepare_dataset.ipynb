{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "For these tutorials, we need to install some additional libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip these if you have already install these libraries\n",
    "!pip install \"nltk\"\n",
    "!pip install \"gensim\"\n",
    "!pip install \"thefuzz[speedup]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Resources for English words (these will need to change if language is different):\n",
    "from nltk.corpus import words, brown\n",
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy string matching for OCR\n",
    "\n",
    "A well-known problem in the digital humanities community is OCR (optical character recognition), which introduces varying degrees of error to the digitized text. OCR-produced text can range from gibberish to almost perfect text, often somewhere in between.\n",
    "\n",
    "This notebook describes **how DeezyMatch could be used for performing fuzzy string matching between OCR queries and a set of candidates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image, the left side shows the human-corrected version of an OCRed fragment:\n",
    "\n",
    "![](images/ocr_aligment.png)\n",
    "\n",
    "Source: **[HERE THE SOURCE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR-induced errors are due to misrecognition of characters, such as `h` recognized as `b`, or `c` regotnized as `o`. These transformations between similarly-looking characters are largely generic (e.g. ‘e’ to ‘c’, or ‘B’ to ‘P’), but different typographies or OCR softwares may lead to dataset-specific errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR and word embeddings\n",
    "\n",
    "This issue becomes obvious when we look at the top nearest neighbours of a specific word, using word embeddings trained on digitized data. Word embeddings (such as those trained with [Word2vec](https://en.wikipedia.org/wiki/Word2vec)) are based on the distributional hypothesis, which is summarized \"You shall tell a word by the company it keeps\" or, in other word, words that are used and occur in the same contexts tend to purport similar meanings (see [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)).\n",
    "\n",
    "With word embeddings trained on clean text, the top nearest neighbours are words that are semantically similar or almost interchangeable. With word embeddings trained on OCRed text, very often the top nearest neighbours are just OCR variations of the target token, as can be seen in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cmp_machineNN.png)\n",
    "\n",
    "**Source:** `glove-wiki-gigaword-50` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/OCR_machineNN.png)\n",
    "\n",
    "**Source:** word embeddings trained from Living with Machines newspapers, by Nilo Pedrazzini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and use DeezyMatch to find candidates for OCRed queries, we need to prepare the following datasets:\n",
    "* **String matching dataset:** dataset of string pairs with positive and negative matches (e.g. `maciiine` is  an OCR variation of `machine`, `maciiine` is not an OCR variation of `device`). In the following cells, we show how to build a dataset of string pairs using word embeddings trained on digitized text. \n",
    "\n",
    "The idea is the following: \n",
    "\n",
    "- if the string similarity between the nearest neighbour(s) and the target word is very high AND if one is not a subset of the other (such as `machine` and `machines`), in most cases, it will be a string variation of the target word (we have tested this empirically); \n",
    "- if the string similarity between the nearest neighbour and the target word is low (such as `maciiine` and `device`), it will probably be a synonym or near-synonym, and therefore not a string variation. \n",
    "\n",
    "We use this intuition to construct a dataset of positive and negative matches. See some examples for the target word `would`:\n",
    "\n",
    "  ```\n",
    "  would\t  might     FALSE\n",
    "  would\t  must      FALSE\n",
    "  would\t  likely    FALSE\n",
    "  would\t  thought   FALSE\n",
    "  would\t  woull     TRUE\n",
    "  would\t  wonld     TRUE\n",
    "  would\t  woubl     TRUE\n",
    "  would\t  wouid     TRUE\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Candidates dataset:** in this example, list of words in English (obtained from a dictionary and the Brown corpus).\n",
    "* **Queries dataset:** in this example, list of OCRed words.\n",
    "\n",
    "We provide the [vocabulary file](https://github.com/Living-with-machines/DeezyMatch/examples/ocr_with_w2v/inputs/characters_v001.vocab) and the [input file](https://github.com/Living-with-machines/DeezyMatch/examples/ocr_with_w2v/inputs/input_dfm.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the word2vec models\n",
    "\n",
    "This experiment is with word embeddings trained on digitized British newspapers published between 1860 and 1870. Unfortunately, we can't yet share our embeddings. \n",
    "\n",
    "However, we have word embeddings trained on a large historical dataset of books in English, published between 1760 and 1900. See the data paper here: http://doi.org/10.5334/johd.48 or the GitHub repository here: https://github.com/Living-with-machines/histLM.\n",
    "\n",
    "You can download `word2vec.zip` from [zenodo](https://zenodo.org/record/4782245#.YtZ7zezML0o). After unzipping the file, move the directory to `data/`, the `ocr_with_w2v` directory should now look similar to this:\n",
    "\n",
    "### Directory structure\n",
    "\n",
    "```\n",
    "ocr_with_w2v\n",
    "├── README.md\n",
    "├── data\n",
    "│   └── word2vec\n",
    "│       ├── w2v_1760_1850\n",
    "│       │   ├── w2v_words.model\n",
    "│       │   ├── w2v_words.model.trainables.syn1neg.npy\n",
    "│       │   └── w2v_words.model.wv.vectors.npy\n",
    "│       └── w2v_1760_1900\n",
    "│           ├── w2v_words.model\n",
    "│           ├── w2v_words.model.trainables.syn1neg.npy\n",
    "│           └── w2v_words.model.wv.vectors.npy\n",
    "├── images\n",
    "│   └── ...\n",
    "├── inputs\n",
    "│   ├── characters_v001.vocab\n",
    "│   └── input_dfm.yaml\n",
    "├── prepare_dataset.ipynb\n",
    "└── tutorial_ocr_w2v.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other word embeddings\n",
    "\n",
    "You can try with other pretrained word embeddings in digitized newspapers in other languages, such as:\n",
    "\n",
    "* https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.22/\n",
    "* https://zenodo.org/record/4892800#.YtKldsHMLeo\n",
    "\n",
    "Download a model and store it as `data/[model_name]/w2v.model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the string pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder name of the w2v model:\n",
    "model_name = \"w2v_1760_1900\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the word2vec model\n",
    "path2model = os.path.join(\"data\", model_name, \"w2v_words.model\")\n",
    "model = Word2Vec.load(path2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words in the embeddings:\n",
    "w2v_words = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words in the English language:\n",
    "nltk_words = set(words.words())\n",
    "brown_words = set(brown.words())\n",
    "glove_words = set(glove_vectors.index_to_key)\n",
    "\n",
    "english_words = nltk_words.union(brown_words).union(glove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_matches(word, sims, fuzz_ratio_threshold=50):\n",
    "    \"\"\"\n",
    "    Given a word and the top 100 nearest neighbours, separate into positive and negative matches.\n",
    "    \n",
    "    Arguments:\n",
    "        word (str): a word.\n",
    "        sims (list): the list of 100 nearest neighbours from the OCR word2vec model.\n",
    "        fuzz_ratio_threshold (float): threshold for fuzz.ratio\n",
    "            If the nearest neighbour word is a word of the English language\n",
    "            and the string similarity is less than fuzz_ratio_threshold, we consider it a\n",
    "            negative match (i.e. not an OCR variation)\n",
    "        \n",
    "    Returns:\n",
    "        positive (list): a list of postive matches.\n",
    "        negative (list): a list of negative matches.\n",
    "    \"\"\"\n",
    "    negative = []\n",
    "    positive = [word]\n",
    "    for nn in sims:\n",
    "        nn_word = nn[0]\n",
    "        # If one word is not a subset of another:\n",
    "        if not nn_word in word and not word in nn_word:\n",
    "            # Split both the word and the nearest neighbour in two parts: the idea is that both\n",
    "            # parts should be equally similar or equally dissimilar, in order to consider them\n",
    "            # as positive or negative matches (e.g. \"careless\" and \"listless\" are two clear words\n",
    "            # but have high string similarity due to a big chunk of the word---the suffix---being\n",
    "            # identical):\n",
    "            nn_word_1 = nn_word[:len(nn_word)//2]\n",
    "            nn_word_2 = nn_word[len(nn_word)//2:]\n",
    "            word_1 = word[:len(word)//2]\n",
    "            word_2 = word[len(word)//2:]\n",
    "            # If the nearest neighbour word is a word of the English language\n",
    "            # and the string similarity is less than 0.50, we consider it a\n",
    "            # negative match (i.e. not an OCR variation):\n",
    "            if nn_word in english_words and fuzz.ratio(nn_word_1, word_1) < fuzz_ratio_threshold and fuzz.ratio(nn_word_2, word_2) < fuzz_ratio_threshold:\n",
    "                negative.append(nn_word)\n",
    "            # If the nearest neighbour word is not a word of the English language\n",
    "            # and the string similarity is more than 0.50, we consider it a\n",
    "            # positive match (i.e. an OCR variation):\n",
    "            if not nn_word in english_words and fuzz.ratio(nn_word_1, word_1) > fuzz_ratio_threshold and fuzz.ratio(nn_word_2, word_2) > fuzz_ratio_threshold:\n",
    "                positive.append(nn_word)\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter w2v_words\n",
    "\n",
    "It takes a bit of time to create positive/negative matches for all words in `w2v_words`. For this tutorial, we first reduce the number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter w2v_words\n",
    "w2v_words = [x for x in w2v_words if 4 <= len(x) <= 7]\n",
    "w2v_words = w2v_words[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word in the w2v model, keep likely positive and negative matches:\n",
    "positive_matches = []\n",
    "negative_matches = []\n",
    "for word in tqdm(w2v_words):\n",
    "    # For each word in the w2v model that is longer than 4 characters and \n",
    "    # is a word in the English language:\n",
    "    if len(word) > 4 and word in english_words:\n",
    "        # Get the top 100 nearest neighbors\n",
    "        sims = model.wv.most_similar(word, topn=100)\n",
    "        # Distinguist between positive and negative matches, where\n",
    "        # * a positive match is an OCR word variation\n",
    "        # * a negative match is a different word\n",
    "        positive, negative = obtain_matches(word, sims)\n",
    "        # We should have the same number of positive matches as negative:\n",
    "        shortest_length = min([len(positive), len(negative)])\n",
    "        negative = negative[:shortest_length]\n",
    "        positive = positive[:shortest_length]\n",
    "        # Prepare for writing into file:\n",
    "        negative_matches += [word + \"\\t\" + x + \"\\t\" + \"FALSE\\n\" for x in negative]\n",
    "        positive_matches += [word + \"\\t\" + x + \"\\t\" + \"TRUE\\n\" for x in positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write string pairs into a file:\n",
    "with open(os.path.join(\"data\", f\"w2v_ocr_pairs_{model_name}.txt\"), \"w\") as fw:\n",
    "    for nm in negative_matches:\n",
    "        fw.write(nm)\n",
    "    for pm in positive_matches:\n",
    "        fw.write(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", f\"w2v_ocr_pairs_{model_name}.txt\")) as fr:\n",
    "    pairs = fr.readlines()\n",
    "\n",
    "# Write queries into a file (in this case, all words in the OCR word2vec model vocabulary\n",
    "# if they are in the list of English words):\n",
    "with open(os.path.join(\"data\", f\"candidates_{model_name}.txt\"), \"w\") as fw:\n",
    "    candidates_set = set()\n",
    "    for p in pairs:\n",
    "        candidate = p.split(\"\\t\")\n",
    "        if candidate[0] in english_words and (candidate[0] not in candidates_set):\n",
    "            fw.write(candidate[0] + \"\\n\")\n",
    "            candidates_set.add(candidate[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the queries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", f\"w2v_ocr_pairs_{model_name}.txt\")) as fr:\n",
    "    pairs = fr.readlines()\n",
    "\n",
    "# Write queries into a file (in this case, all words in the OCR word2vec model vocabulary\n",
    "# if they are not in the list of English words):\n",
    "with open(os.path.join(\"data\", f\"queries_{model_name}.txt\"), \"w\") as fw:\n",
    "    for p in pairs:\n",
    "        query = p.split(\"\\t\")\n",
    "        if len(query[1]) > 4 and (not query[1] in english_words):\n",
    "            fw.write(query[1] + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38deezy)",
   "language": "python",
   "name": "py38deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
